{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f56cd67",
   "metadata": {},
   "source": [
    "# Randeng-Pegasus 模型微调测试\n",
    "\n",
    "本notebook用于测试LCSTS数据集的加载和Randeng-Pegasus模型的微调准备工作。\n",
    "\n",
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "from src.data.dataset import LCSTSDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab8ac4",
   "metadata": {},
   "source": [
    "## 2. 数据集加载与验证\n",
    "\n",
    "首先加载LCSTS数据集并查看样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770656e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集文件\n",
    "data_path = 'data/lcsts/lcsts_data.json'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f'数据集大小: {len(data)}条')\n",
    "print('\\n示例数据:')\n",
    "for i in range(3):\n",
    "    print(f'\\n例{i+1}:')\n",
    "    print(f'标题: {data[i][\"title\"]}')\n",
    "    print(f'内容: {data[i][\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25902a",
   "metadata": {},
   "source": [
    "## 3. 模型与分词器初始化\n",
    "\n",
    "使用本地的Randeng-Pegasus模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d84d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化分词器和模型\n",
    "model_name = \"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = LCSTSDataset(\n",
    "    data_path=data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_source_length=512,\n",
    "    max_target_length=128\n",
    ")\n",
    "\n",
    "# 测试数据集的第一个样本\n",
    "sample = dataset[0]\n",
    "print('数据集样本格式:')\n",
    "for key, value in sample.items():\n",
    "    print(f'{key}: shape {value.shape}')\n",
    "\n",
    "# 解码测试\n",
    "print('\\n解码测试:')\n",
    "decoded_source = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "decoded_target = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "print(f'源文本: {decoded_source}')\n",
    "print(f'目标摘要: {decoded_target}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
