{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0b7d62",
   "metadata": {},
   "source": [
    "# Randeng-Pegasus 模型微调测试\n",
    "\n",
    "本notebook用于测试LCSTS数据集的加载和Randeng-Pegasus模型的微调准备工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.data.dataset import LCSTSDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0861b",
   "metadata": {},
   "source": [
    "## 1. 加载数据集\n",
    "\n",
    "首先测试LCSTS数据集的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集文件\n",
    "data_path = '../data/lcsts/lcsts_data.json'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"数据集大小: {len(data)}条\")\n",
    "print(\"\\n示例数据:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n例{i+1}:\")\n",
    "    print(f\"标题: {data[i]['title']}\")\n",
    "    print(f\"内容: {data[i]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440331d3",
   "metadata": {},
   "source": [
    "## 2. 测试数据集类\n",
    "\n",
    "测试我们的LCSTSDataset类是否正常工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化tokenizer\n",
    "model_name = \"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = LCSTSDataset(\n",
    "    data_path=data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_source_length=512,\n",
    "    max_target_length=128\n",
    ")\n",
    "\n",
    "# 测试数据集的第一个样本\n",
    "sample = dataset[0]\n",
    "print(\"数据集样本格式:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: shape {value.shape}\")\n",
    "\n",
    "# 解码测试\n",
    "print(\"\\n解码测试:\")\n",
    "decoded_source = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "decoded_target = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "print(f\"源文本: {decoded_source}\")\n",
    "print(f\"目标摘要: {decoded_target}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
